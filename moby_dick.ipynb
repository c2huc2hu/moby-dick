{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab descriptive statistics about the corpus\n",
    "file = open('whale2.txt')\n",
    "full_text = file.read()\n",
    "import collections\n",
    "\n",
    "character_set = set(full_text)\n",
    "character_counts = collections.Counter(full_text)\n",
    "print(''.join(([k for k, v in character_counts.most_common()])))\n",
    "print(sorted(list(character_set)), len(character_set)) # 81 unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1215235,)\n",
      "(1215235, 1, 71)\n",
      "(75952, 16) (75952, 16, 71)\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 16 # size of recurrent state. number of parameters grows quadratically with this\n",
    "EMBED_SIZE = 16 # size of embedding. number of parameters grows linearly with this\n",
    "NUM_CHARS = 71 # only use the NUM_CHARS most common characters. reduces the number of parameters needed in the embedding, and probably improves accuracy\n",
    "\n",
    "# character level language model\n",
    "# text = \"abc abcd abcdef abcdefghijk abc ab abc abc abcd ab\" # sample input for testing\n",
    "mobydick = open('whale2.txt')\n",
    "text = mobydick.read()\n",
    "\n",
    "# one-hot code the text\n",
    "chars = ''' etaonsihrldumcwgf,ypbvk.-\\n;I\"'ATS!HBWEqNCPx?OLjRFMDGzYQJU():KV1028573*4Z69X_$][&'''\n",
    "char_to_idx = dict(zip(chars[:NUM_CHARS], range(len(chars))))\n",
    "idx_to_char = dict(zip(range(len(chars)), chars[:NUM_CHARS]))\n",
    "\n",
    "# create input and output tensors to learn from. we never output the first character or input the last char\n",
    "input_ = np.array([char_to_idx.get(ch, 0) for ch in text], dtype=np.int32)\n",
    "output = keras.utils.to_categorical(input_[1:], NUM_CHARS)\n",
    "output = np.expand_dims(output, axis=1)\n",
    "input_ = input_[:-1]\n",
    "print(input_.shape)\n",
    "print(output.shape)\n",
    "\n",
    "# splice the full sequence into shorter sequences for training\n",
    "training_seq_len = 16\n",
    "if len(input_) % training_seq_len == 0:\n",
    "    np.append(input_, 0)\n",
    "training_input = input_[:len(input_) // training_seq_len * training_seq_len].reshape(\n",
    "    (-1, training_seq_len))\n",
    "training_output = output[:len(input_) // training_seq_len * training_seq_len,:,:].reshape(\n",
    "    (-1, training_seq_len, NUM_CHARS))\n",
    "\n",
    "print(training_input.shape, training_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (1, 16, 16)               1136      \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (1, 16, 16)               1584      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (1, 16, 71)               1207      \n",
      "=================================================================\n",
      "Total params: 3,927\n",
      "Trainable params: 3,927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(NUM_CHARS, EMBED_SIZE, batch_input_shape=(1, training_seq_len)))\n",
    "for i in range(1):\n",
    "    model.add(keras.layers.GRU(\n",
    "        HIDDEN_SIZE, return_sequences=True, stateful=True, unroll=True))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(len(chars) - filter_chars, activation='softmax')))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "75952/75952 [==============================] - 407s 5ms/step - loss: 2.3175\n"
     ]
    }
   ],
   "source": [
    "model.fit(training_input, training_output, batch_size=1,\n",
    "          epochs=1, shuffle=False) # , validation_data=(input_, output), validation_freq=10)  # supported on master, not on latest stable\n",
    "model.save(\n",
    "    './moby-dick.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference\n",
    "model.reset_states()\n",
    "test_input = np.array([char_to_idx.get(ch, 0) for ch in 'abc'], dtype=np.int32)\n",
    "test_input = np.expand_dims(test_input, axis=0)\n",
    "result = model.predict(test_input)\n",
    "result_text = np.argmax(result, axis=2)\n",
    "print([idx_to_char[ch] for ch in result_text.flatten().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(ch):\n",
    "    '''predict the next character given the previous one'''\n",
    "    test_input = np.array([char_to_idx.get(ch, 0)])\n",
    "    test_input = np.expand_dims(test_input, axis=0)\n",
    "    result = model.predict(test_input)\n",
    "    result_text = np.argmax(result, axis=2)\n",
    "    return idx_to_char[result_text.flatten()[0]]\n",
    "\n",
    "infer('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "keras.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
